{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create Confusion table\n",
    "\n",
    "First we had double peaked field's area that was greater than double cropped. Then we filtered out the orchards and irrelevant fields. Then the area of double-peacked dropped below area of double-cropped.\n",
    "\n",
    "Then we ran the code for several parameters for Grant 2016 and 2017 and now we want to create confusion table to see which parameters are the best, using Grant 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "from IPython.display import Image\n",
    "from shapely.geometry import Point, Polygon\n",
    "from math import factorial\n",
    "import datetime\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "import os, os.path\n",
    "\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from patsy import cr\n",
    "\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import core module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# search path for modules\n",
    "# look @ https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\n",
    "sys.path.append('/Users/hn/Documents/00_GitHub/Ag/remote_sensing/python/')\n",
    "import remote_sensing_core as rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_base = \"/Users/hn/Documents/01_research_data/remote_sensing/02_peaks_and_plots/Grant_2016/csv/\"\n",
    "param_dir = \"/Users/hn/Documents/00_GitHub/Ag/remote_sensing/parameters/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grant 2016 Time Series \n",
    "which includes all polygons in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grant_2016_TS = pd.read_csv(\"/Users/hn/Documents/01_research_data/\" + \\\n",
    "                            \"remote_sensing/02_peaks_and_plots/Grant_2016/Grant_2016_TS.csv\")\n",
    "\n",
    "# drop image columns\n",
    "Grant_2016_TS.drop([\"system:index\", \"B2\" , \"B3\", \"B4\", \"B8\", \"doy\", \"NDVI\"], axis=1, inplace=True)\n",
    "\n",
    "# dropping ALL duplicte values \n",
    "Grant_2016_TS.drop_duplicates(inplace = True) \n",
    "\n",
    "# remane .geo column to geo\n",
    "Grant_2016_TS.rename(columns={\".geo\": \"geo\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grant_2016_TS.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grant_2016_TS[\"geo\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of files\n",
    "    - Extract list of files in the input directory\n",
    "    - Filter the wanted files that contain \"all_polygons\" in their name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(data_dir_base)\n",
    "file_list = [k for k in file_list if 'all_polygons' in k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form confusion table\n",
    "\n",
    "    - define \"double\" as yes. Predicted Yes (double) and actual (yes) double-cropped:\n",
    "    \n",
    "    ----------------------------------------------------------------\n",
    "    |                  |                  |                        |\n",
    "    |                  | Predicted double |  Predicted NOT double  |\n",
    "    |                  |      2 peaks     |       !(2 peaks)       |\n",
    "    ----------------------------------------------------------------\n",
    "    |                  |                  |                        |\n",
    "    | Actual double    |        TP        |           FN           |\n",
    "    |                  |                  |                        |\n",
    "    ----------------------------------------------------------------\n",
    "    |                  |                  |                        |\n",
    "    | Actual NOT double|       FP         |          TN            |\n",
    "    |                  |                  |                        |\n",
    "    ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns = ['col', 'predicted_double_peak', 'predicted_not_double_peak',\n",
    "                  'total_sum', 'params', \"ignored_fields_count\"]\n",
    "\n",
    "all_confusions = pd.DataFrame(data=None, \n",
    "                              index=np.arange(len(file_list)*2), \n",
    "                              columns=output_columns)\n",
    "pointer = 0\n",
    "all_confusions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_columns = ['col', 'predicted_double_peak', 'predicted_not_double_peak',\n",
    "                  'total_sum', 'params', \"ignor\"]\n",
    "\n",
    "all_confusions = pd.DataFrame(data=None, \n",
    "                              index=np.arange(len(file_list)*2), \n",
    "                              columns=output_columns)\n",
    "pointer = 0\n",
    "\n",
    "for file in file_list:\n",
    "    broken_pieces = file.split(\"_\")\n",
    "    a_data = pd.read_csv(data_dir_base + file)\n",
    "    \n",
    "    # remove the last row\n",
    "    a_data = a_data[0:(a_data.shape[0]-1)]\n",
    "    double_cropped, NotDouble_cropped = rc.divide_double_nonDouble_by_notes(a_data)\n",
    "    double_crop_double_peak, double_crop_NotDouble_peak = rc.divide_double_nonDouble_peaks(double_cropped)\n",
    "    NotDouble_cropped_double_peak, NotDouble_cropped_NotDouble_peak = rc.divide_double_nonDouble_peaks(NotDouble_cropped)\n",
    "    \n",
    "    ############################################################\n",
    "    ###\n",
    "    ###     Form the confusion matrix\n",
    "    ###\n",
    "    ############################################################\n",
    "    params = broken_pieces[0] + \" = \" + broken_pieces[1] + \", \" + broken_pieces[2] + \" = \" + broken_pieces[3]\n",
    "    TP = double_crop_double_peak.shape[0]\n",
    "    FN = double_crop_NotDouble_peak.shape[0]\n",
    "    FP = NotDouble_cropped_double_peak.shape[0]\n",
    "    TN = NotDouble_cropped_NotDouble_peak.shape[0]\n",
    "    total_size = TP + TN + FP + FN\n",
    "    ignored_fields_count = Grant_2016_TS.shape[0] - a_data.shape[0]\n",
    "    d = {'col' : [\"Actual double-cropped\", \"actual not-double-cropped\"], \n",
    "         'predicted_double_peak': [TP, FP],\n",
    "         'predicted_not_double_peak': [FN, TN],\n",
    "         'total_sum': [total_size, total_size],\n",
    "         'params': [params, params],\n",
    "         \"ignored_fields_count\":[ignored_fields_count, ignored_fields_count ]\n",
    "        }\n",
    "    curr_confusion = pd.DataFrame(data=d)\n",
    "    \n",
    "    all_confusions.iloc[pointer:(pointer+2)] = curr_confusion.values\n",
    "    pointer += 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_confusions.sort_values(by=['params'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name = \"/Users/hn/Desktop/all_confusions_2016.csv\"\n",
    "all_confusions.to_csv(output_file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =  file_list[1]\n",
    "a_data = pd.read_csv(data_dir_base + file)\n",
    "a_data = a_data[0:(a_data.shape[0]-1)]\n",
    "print(a_data.shape)\n",
    "\n",
    "double_cropped, NotDouble_cropped = rc.divide_double_nonDouble_by_notes(a_data)\n",
    "double_crop_double_peak, double_crop_NotDouble_peak = rc.divide_double_nonDouble_peaks(double_cropped)\n",
    "NotDouble_cropped_double_peak, NotDouble_cropped_NotDouble_peak = rc.divide_double_nonDouble_peaks(NotDouble_cropped)\n",
    "TP = double_crop_double_peak.shape[0]\n",
    "FN = double_crop_NotDouble_peak.shape[0]\n",
    "FP = NotDouble_cropped_double_peak.shape[0]\n",
    "TN = NotDouble_cropped_NotDouble_peak.shape[0]\n",
    "\n",
    "print(\"TP = \" + str(TP))\n",
    "print(\"FP = \" + str(FP))\n",
    "print(\"FN = \" + str(FN))\n",
    "print(\"TN = \" + str(TN))\n",
    "\n",
    "print(\"Number of double-cropped fields is\", str(TP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = TP + TN + FP + FN\n",
    "\n",
    "d = {'col' : [\"Actual double-cropped\", \"actual not-double-cropped\"], \n",
    "     'predicted_double_peak': [TP, FP],\n",
    "     'predicted_not_double_peak': [FN, TN],\n",
    "     'total_sum': [total_size, total_size],\n",
    "     'params': [params, params]\n",
    "    }\n",
    "curr_confusion = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_a_data = a_data.copy()\n",
    "clean_a_data.drop([\"peak_Doy\", \"peak_value\" , \"peak_count\"], axis=1, inplace=True)\n",
    "\n",
    "# dropping ALL duplicte values \n",
    "clean_a_data.drop_duplicates(inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_a_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_a_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_a_data[\"geo\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "double_crop_double_peak.to_csv(\"/Users/hn/Desktop/double_crop_double_peak.csv\", index = False)\n",
    "double_crop_NotDouble_peak.to_csv(\"/Users/hn/Desktop/double_crop_NotDouble_peak.csv\", index = False)\n",
    "NotDouble_cropped_double_peak.to_csv(\"/Users/hn/Desktop/NotDouble_cropped_double_peak.csv\", index = False)\n",
    "NotDouble_cropped_NotDouble_peak.to_csv(\"/Users/hn/Desktop/NotDouble_cropped_NotDouble_peak.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = broken_pieces[0] + \" = \" + broken_pieces[1] + \", \" + broken_pieces[2] + \" = \" + broken_pieces[3]\n",
    "TP = double_crop_double_peak.shape[0]\n",
    "FN = double_crop_NotDouble_peak.shape[0]\n",
    "FP = NotDouble_cropped_double_peak.shape[0]\n",
    "TN = NotDouble_cropped_NotDouble_peak.shape[0]\n",
    "total_size = TP + TN + FP + FN\n",
    "\n",
    "d = {'col' : [\"Actual double-cropped\", \"actual not-double-cropped\"], \n",
    "     'predicted_double_peak': [TP, FP],\n",
    "     'predicted_not_double_peak': [FN, TN],\n",
    "     'total_sum': [total_size, total_size],\n",
    "     'params': [params, params]\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_data.Notes.unique()\n",
    "# a_data.to_csv(\"/Users/hn/Desktop/a_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
